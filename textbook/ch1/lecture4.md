**CIDS 161: Introduction to Information and Computing**

### **Week 2: Understanding Information and Its Role in Computing**

#### **Defining Information**
When presented with a coin flip and asked to choose heads or tails, you initially do not know the answer. This represents **uncertainty**. Once the outcome is revealed (e.g., tails), you have gained **information**—the resolution of uncertainty.

Information, in a mathematical sense, can be understood as the **answer to a yes or no question**. This fundamental definition plays a crucial role in computing and data processing.

#### **Foundations of Information Theory**
The study of information was formalized through **information theory**, pioneered by researchers such as:
- **Harry Nyquist** and **Ralph Hartley** (early 20th century)
- **Claude Shannon**, the "father of information theory" (1940s)

These theorists worked on making information tangible—something measurable, computable, and transmittable, forming the basis of modern computing.

#### **Information vs. Data**
- **Data**: Raw observations or measurements (e.g., "It is 25 degrees outside").
- **Information**: Data that resolves uncertainty (e.g., answering "Is it below freezing?" with "No").

Data becomes useful when structured in a way that answers yes-or-no questions, leading to information.

### **Measuring Information**
Information can be quantified. **Shannon's Information Theory** defines it using:
- **Entropy (H)**: A measure of uncertainty (higher entropy = more uncertainty to resolve).
- **Bits (binary digits)**: The fundamental unit of information, representing one yes/no (1 or 0) answer.

#### **The Bit and Digital Representation**
- A **bit** represents a binary choice (on/off, 1/0, yes/no).
- A **byte** is a collection of 8 bits.
- Storage and data are measured in bytes, kilobytes (KB), megabytes (MB), gigabytes (GB), etc.

Example: A **light switch** represents a simple binary state: **on (1) or off (0)**.

Computers use binary because it is reliable for electronic components to distinguish between two distinct states, allowing for stable storage and processing of data.

### **Understanding Entropy and Information Processing**
**Entropy** determines how much information is needed to resolve uncertainty.
- A coin flip (heads/tails) has **1 bit** of entropy (1 yes/no question needed).
- A six-sided die requires **more bits** (more yes/no questions) to determine its outcome.

### **The Structure of a Computer**
A modern computer follows the **Von Neumann Architecture**, consisting of:
1. **Input Devices** (e.g., keyboard, mouse, microphone) – gather data.
2. **Processor (CPU)** – performs calculations and makes decisions.
3. **Memory & Storage**:
   - **RAM** (Random Access Memory) – short-term, fast storage.
   - **Hard Drives (HDDs) & SSDs** – long-term storage.
4. **Output Devices** (e.g., screen, speakers, printer) – present results.

Computers function by taking **inputs**, processing them using **stored data**, and producing useful **outputs**.

### **Conclusion: What is a Computer?**
A **computer** is a device that **stores and processes information**, enabling decision-making based on inputs and past data. While traditionally electronic, the concept of computation extends to non-digital devices and even human computation.

### **Next Steps**
- Exploring **programming** and how we use software to instruct computers.
- Understanding **how computers process inputs to generate meaningful outputs**.

---
**Discussion Questions:**
1. **How does the distinction between data and information help in computer science?**
2. **Why is binary (bits) used as the fundamental unit of information in computing?**
3. **How does the concept of entropy relate to problem-solving in computing?**

